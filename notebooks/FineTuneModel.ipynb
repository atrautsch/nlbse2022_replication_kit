{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cfb69ff-1771-4b3d-a10e-bc57cdab191a",
   "metadata": {},
   "source": [
    "# Fine-tune the pre-trained seBERT Model\n",
    "In this notebook we fine-tune the previously pre-trained seBERT model for the task of classifying issues into bug, enhancement and question.\n",
    "\n",
    "It requires the pre-trained model in ./models/ and the gunzipped data in ./data/ as described in the README.md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc9da8-7ad5-4e36-84b4-f04abdf654d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, matthews_corrcoef, accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51a75e-aa5a-47da-b950-b7e77ac473d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Just a standard torch Dataset for BERT-style data.\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f7d42-098a-4955-acce-3cd9cc494257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_multi_label(p):\n",
    "    \"\"\"This metrics computation is used by the huggingface trainer.\"\"\"\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    mcc = matthews_corrcoef(y_true=labels, y_pred=pred)\n",
    "\n",
    "    recall_ma = recall_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    precision_ma = precision_score(y_true=labels, y_pred=pred, average='macro')\n",
    "    f1_ma = f1_score(y_true=labels, y_pred=pred, average='macro')\n",
    "\n",
    "    return {'accuracy': accuracy, 'precision_micro': precision, 'recall_micro': recall, 'f1_micro': f1, 'mcc': mcc, 'precision_macro': precision_ma, 'recall_macro': recall_ma, 'f1_macro': f1_ma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245950ab-3657-499a-9204-2ece64035aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class seBERT(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    We are effectively wrapping the high-level Trainer and TrainingArguments classes from the Huggingface library into a\n",
    "    scikit-learn classifier.\n",
    "    This allows us to use all of scikit-learn in a more natural way, e.g., pipelines or grid search.\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoints_dir='../checkpoints/', batch_size=8):\n",
    "        self.trainer = None\n",
    "        self.checkpoints_dir = checkpoints_dir\n",
    "        self.model = BertForSequenceClassification.from_pretrained('../models/seBERT/pytorch_model.bin', config='../models/seBERT/config.json', num_labels=3)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('../models/seBERT/', do_lower_case=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = 128\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Scikit-learn fit implements simple fine-tuning from the pre-trained model.\n",
    "\n",
    "        We split the training data into 80/20 training and validation sets, train for 5 epochs and chose the model\n",
    "        that performs best on the validation data in the end.\n",
    "        \"\"\"\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "        X_train_tokens = self.tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=self.max_length)\n",
    "        X_val_tokens = self.tokenizer(X_val.tolist(), padding=True, truncation=True, max_length=self.max_length)\n",
    "\n",
    "        train_dataset = Dataset(X_train_tokens, y_train)\n",
    "        eval_dataset = Dataset(X_val_tokens, y_val)\n",
    "\n",
    "        if not os.path.exists(self.checkpoints_dir):\n",
    "            os.makedirs(self.checkpoints_dir)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir                  = self.checkpoints_dir,\n",
    "            num_train_epochs            = 5,\n",
    "            per_device_train_batch_size = self.batch_size,\n",
    "            per_device_eval_batch_size  = self.batch_size,\n",
    "            gradient_accumulation_steps = 4,\n",
    "            eval_accumulation_steps     = 10,\n",
    "            evaluation_strategy         = 'epoch',\n",
    "            save_strategy               = 'epoch',\n",
    "            load_best_model_at_end      = True\n",
    "        )\n",
    "        self.trainer = Trainer(\n",
    "            model           = self.model,\n",
    "            args            = training_args,\n",
    "            train_dataset   = train_dataset,\n",
    "            eval_dataset    = eval_dataset,\n",
    "            compute_metrics = compute_metrics_multi_label\n",
    "        )\n",
    "        print(self.trainer.train())\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X, y=None):\n",
    "        \"\"\"This is kept simple intentionally, for larger Datasets this would be too ineficient,\n",
    "        because we would effectively force a batch size of 1.\"\"\"\n",
    "        y_probs = []\n",
    "        self.trainer.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _, X_row in enumerate(X):\n",
    "                inputs = self.tokenizer(X_row, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\").to('cuda')\n",
    "                outputs = self.trainer.model(**inputs)\n",
    "                probs = outputs[0].softmax(1).cpu().detach().numpy()\n",
    "                y_probs.append(probs)\n",
    "        return y_probs\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"Predict is evaluation.\"\"\"\n",
    "        y_probs = self.predict_proba(X, y)\n",
    "        y_pred = []\n",
    "        for y_prob in y_probs:\n",
    "            y_pred.append(y_prob.argmax())\n",
    "        return y_pred\n",
    "\n",
    "    def save_model(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        self.trainer.model.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146d830-ef25-4f4e-9a73-977dca628670",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = seBERT()\n",
    "\n",
    "# load the provided traianing data\n",
    "df = pd.read_csv('../data/github-labels-top3-803k-train.csv')\n",
    "\n",
    "# preprocess it\n",
    "def preprocess(row):\n",
    "    ret = str(row['issue_title']) + \" \" + str(row['issue_body'])\n",
    "    ret = gensim.parsing.preprocessing.strip_multiple_whitespaces(ret)\n",
    "    ret = ret.replace('\\r\\n', ' ')\n",
    "    ret = ret.replace('\\n', ' ')\n",
    "    return ret\n",
    "\n",
    "# make labels numeric\n",
    "def labelnum(row):\n",
    "    if row['issue_label'] == 'bug':\n",
    "        return 0\n",
    "    elif row['issue_label'] == 'enhancement':\n",
    "        return 1\n",
    "    elif row['issue_label'] == 'question':\n",
    "        return 2\n",
    "    else:\n",
    "        raise Exception('no such type!')\n",
    "\n",
    "df['text_no_newlines'] = df.apply(preprocess, axis=1)\n",
    "df['label'] = df.apply(labelnum, axis=1)\n",
    "\n",
    "X = df['text_no_newlines'].values\n",
    "y = df['label'].astype(int).values\n",
    "\n",
    "# fit all the data\n",
    "clf.fit(X, y)\n",
    "\n",
    "# save the fine-tuned model\n",
    "clf.save_model('../models/nlbse/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7ba8c-44b6-4c85-9551-b63ae917331f",
   "metadata": {},
   "source": [
    "## Smaller sample fine-tuning\n",
    "We have reduced the batch_size and sample size so that this should be runnable on consumer hardware (tested on NVIDA GTX 1080).\n",
    "Note that the checkpointing of the model requires space nonetheless, it should be ~35GB in this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5a376-2c68-4d0c-805d-253d3a8ef8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = seBERT(batch_size=4)\n",
    "\n",
    "# load the provided traianing data\n",
    "df = pd.read_csv('../data/github-labels-top3-803k-train.csv').sample(n=100)\n",
    "\n",
    "# preprocess it\n",
    "def preprocess(row):\n",
    "    ret = str(row['issue_title']) + \" \" + str(row['issue_body'])\n",
    "    ret = gensim.parsing.preprocessing.strip_multiple_whitespaces(ret)\n",
    "    ret = ret.replace('\\r\\n', ' ')\n",
    "    ret = ret.replace('\\n', ' ')\n",
    "    return ret\n",
    "\n",
    "# make labels numeric\n",
    "def labelnum(row):\n",
    "    if row['issue_label'] == 'bug':\n",
    "        return 0\n",
    "    elif row['issue_label'] == 'enhancement':\n",
    "        return 1\n",
    "    elif row['issue_label'] == 'question':\n",
    "        return 2\n",
    "    else:\n",
    "        raise Exception('no such type!')\n",
    "\n",
    "df['text_no_newlines'] = df.apply(preprocess, axis=1)\n",
    "df['label'] = df.apply(labelnum, axis=1)\n",
    "\n",
    "X = df['text_no_newlines'].values\n",
    "y = df['label'].astype(int).values\n",
    "\n",
    "# fit all the data\n",
    "clf.fit(X, y)\n",
    "\n",
    "# save the fine-tuned model\n",
    "# clf.save_model('../models/nlbse/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
