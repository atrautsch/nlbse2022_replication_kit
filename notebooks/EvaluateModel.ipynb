{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f436214e-9956-4c41-a90c-dd8796c1df81",
   "metadata": {},
   "source": [
    "# Evaluate the fine-tuned model\n",
    "In this notebook we evaluate the previously fine-tuned model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c826387c-3b6b-42e8-8c3a-3337a567f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import random\n",
    "import gensim\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, matthews_corrcoef\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "transformers.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbc730-4ce5-4ea8-ad55-7bf70292541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(row):\n",
    "    ret = str(row['issue_title']) + \" \" + str(row['issue_body'])\n",
    "    ret = gensim.parsing.preprocessing.strip_multiple_whitespaces(ret)\n",
    "    ret = ret.replace('\\r\\n', ' ')\n",
    "    ret = ret.replace('\\n', ' ')\n",
    "    return ret\n",
    "\n",
    "def labelnum(row):\n",
    "    if row['issue_label'] == 'bug':\n",
    "        return 0\n",
    "    elif row['issue_label'] == 'enhancement':\n",
    "        return 1\n",
    "    elif row['issue_label'] == 'question':\n",
    "        return 2\n",
    "    else:\n",
    "        raise Exception('no such type!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b52fd-28f2-4115-9c94-46661ce3a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../models/nlbse/'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "df = pd.read_csv('../data/github-labels-top3-803k-test.csv')\n",
    "\n",
    "df['text_no_newlines'] = df.apply(preprocess, axis=1)\n",
    "df['label'] = df.apply(labelnum, axis=1)\n",
    "\n",
    "X = df['text_no_newlines'].values\n",
    "y_test = df['label'].astype(int).values\n",
    "\n",
    "data_len = len(df)\n",
    "y_probs = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, X_row in enumerate(X):\n",
    "        inputs = tokenizer(X_row, padding=True, truncation=True, max_length=128, return_tensors='pt').to('cuda')\n",
    "        outputs = model.to('cuda')(**inputs)\n",
    "        probs = outputs[0].softmax(1).cpu().detach().numpy()\n",
    "        y_probs.append(probs)\n",
    "        \n",
    "        if len(y_probs) % 1000 == 0:\n",
    "            print('{}/{}'.format(len(y_probs), data_len))\n",
    "\n",
    "y_pred = []\n",
    "for y_prob in y_probs:\n",
    "    y_pred.append(y_prob.argmax())\n",
    "\n",
    "results = [{'model': 'seBERT',\n",
    "            'mcc': matthews_corrcoef(y_true=y_test, y_pred=y_pred),\n",
    "            'micro_f1': f1_score(y_true=y_test, y_pred=y_pred, average='micro'),\n",
    "            'micro_precision': precision_score(y_true=y_test, y_pred=y_pred, average='micro'),\n",
    "            'micro_recall': recall_score(y_true=y_test, y_pred=y_pred, average='micro'),\n",
    "            'macro_f1': f1_score(y_true=y_test, y_pred=y_pred, average='macro'),\n",
    "            'macro_precision': precision_score(y_true=y_test, y_pred=y_pred, average='macro'),\n",
    "            'macro_recall': recall_score(y_true=y_test, y_pred=y_pred, average='macro'),\n",
    "            'precision_bug': precision_score(y_true=y_test, y_pred=y_pred, average=None, labels=[0])[0],\n",
    "            'precision_enhancement': precision_score(y_true=y_test, y_pred=y_pred, average=None, labels=[1])[0],\n",
    "            'precision_question': precision_score(y_true=y_test, y_pred=y_pred, average=None, labels=[2])[0],\n",
    "            'recall_bug': recall_score(y_true=y_test, y_pred=y_pred, average=None, labels=[0])[0],\n",
    "            'recall_enhancement': recall_score(y_true=y_test, y_pred=y_pred, average=None, labels=[1])[0],\n",
    "            'recall_question': recall_score(y_true=y_test, y_pred=y_pred, average=None, labels=[2])[0],\n",
    "            'f1_bug': f1_score(y_true=y_test, y_pred=y_pred, average=None, labels=[0])[0],\n",
    "            'f1_enhancement': f1_score(y_true=y_test, y_pred=y_pred, average=None, labels=[1])[0],\n",
    "            'f1_question': f1_score(y_true=y_test, y_pred=y_pred, average=None, labels=[2])[0]}]\n",
    "\n",
    "pprint.pprint(results)\n",
    "\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv('../data/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f9afc-b0c6-4519-922d-f97392a76326",
   "metadata": {},
   "source": [
    "# Smaller sample evaluation\n",
    "We are randomly drawing 1000 instances from the test data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a956b0f-91d2-4ebb-a1cf-8a3f52c3691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../models/nlbse/'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# this is a small sample for quickly running the model\n",
    "df = pd.read_csv('../data/github-labels-top3-803k-test.csv').sample(n=1000)\n",
    "\n",
    "df['text_no_newlines'] = df.apply(preprocess, axis=1)\n",
    "df['label'] = df.apply(labelnum, axis=1)\n",
    "\n",
    "X = df['text_no_newlines'].values\n",
    "y_test = df['label'].astype(int).values\n",
    "\n",
    "\n",
    "y_probs = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _, X_row in enumerate(X):\n",
    "        inputs = tokenizer(X_row, padding=True, truncation=True, max_length=128, return_tensors='pt').to('cuda')\n",
    "        outputs = model.to('cuda')(**inputs)\n",
    "        probs = outputs[0].softmax(1).cpu().detach().numpy()\n",
    "        y_probs.append(probs)\n",
    "\n",
    "y_pred = []\n",
    "for y_prob in y_probs:\n",
    "    y_pred.append(y_prob.argmax())\n",
    "\n",
    "results = [{'model': 'seBERT',\n",
    "            'mcc': matthews_corrcoef(y_true=y_test, y_pred=y_pred),\n",
    "            'micro_f1': f1_score(y_true=y_test, y_pred=y_pred, average='micro'),\n",
    "            'micro_precision': precision_score(y_true=y_test, y_pred=y_pred, average='micro'),\n",
    "            'micro_recall': recall_score(y_true=y_test, y_pred=y_pred, average='micro'),\n",
    "            'macro_f1': f1_score(y_true=y_test, y_pred=y_pred, average='macro'),\n",
    "            'macro_precision': precision_score(y_true=y_test, y_pred=y_pred, average='macro'),\n",
    "            'macro_recall': recall_score(y_true=y_test, y_pred=y_pred, average='macro'),\n",
    "            'precision_bug': precision_score(y_true=y_test, y_pred=y_pred, average=None, labels=[0])[0],\n",
    "            'precision_enhancement': precision_score(y_true=y_test, y_pred=y_pred, average=None, labels=[1])[0],\n",
    "            'precision_question': precision_score(y_true=y_test, y_pred=y_pred, average=None, labels=[2])[0],\n",
    "            'recall_bug': recall_score(y_true=y_test, y_pred=y_pred, average=None, labels=[0])[0],\n",
    "            'recall_enhancement': recall_score(y_true=y_test, y_pred=y_pred, average=None, labels=[1])[0],\n",
    "            'recall_question': recall_score(y_true=y_test, y_pred=y_pred, average=None, labels=[2])[0],\n",
    "            'f1_bug': f1_score(y_true=y_test, y_pred=y_pred, average=None, labels=[0])[0],\n",
    "            'f1_enhancement': f1_score(y_true=y_test, y_pred=y_pred, average=None, labels=[1])[0],\n",
    "            'f1_question': f1_score(y_true=y_test, y_pred=y_pred, average=None, labels=[2])[0]}]\n",
    "\n",
    "pprint.pprint(results)\n",
    "\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv('../data/results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
